#!/bin/bash
echo "Content-type: text/plain"
echo
# Works with plain URLs, not those containing "?", so /[a-zA-Z0-9]/, ".", and "/"
localip="b.lan"
# /etc/hosts looks something like 127.0.0.1 a.lan \ 10.0.0.233 b.lan
basepath="/zd/put/cunt/warc"
pidcount=$(ps -ef | grep -v grep | grep "mem " | wc -l)
if [ $pidcount -gt 5 ]; then
    echo "Too many arc PID(s) detected. Try again later. PID(s):"
    ps -ef | grep -v grep | grep "mem " | grep -n $
    exit 0
else
    url="$(echo -n "$REQUEST_URI" | sed "s/^\/cgi-bin\/mem?url=//g")"
    time="$(TZ=UTC date -u +%Y%m%d%H%M%S)"
    timeu=$(date +%s)
    urlsafe=$(echo "$url" | sed "s/:\|\/\|?\|=\|&\|(\|)\|,\|@\|+\|*\|%\|#/-/g")
    urllen=$(echo -n $time-$urlsafe | wc --bytes)
    if [ $urllen -gt 200 ]; then
        urlsafe="$(echo $urlsafe | perl -pE "s/^(.{200}).*/\1.URL2LONG/g")"
    fi

    mkdir $basepath/../memento/$timeu
    echo -n "url is: "; echo "$url" | tee $basepath/../memento/$timeu/$time-$urlsafe.txt
    echo "urlsafe: $urlsafe"
    echo "time is: $time, $timeu"; echo

    echo "Main CID at the bottom:"
    cd $basepath; TZ=UTC wget --no-check-certificate -p --span-hosts --adjust-extension --convert-links --restrict-file-names=windows --warc-max-size=99123456 --warc-cdx -e robots=off --warc-file=$basepath/../memento/$timeu/$time-$urlsafe --directory-prefix=$basepath/../memento/$timeu/memento/$time/ "$url" 2>/dev/null
    urlpath=$(echo $url | sed "s/^https\?:\/\///g")

#   if [[ ! "$str" =~ index\.html$ ]]; then echo move; fi
#   / = // in path///index.html or path/index.html
    stat -t "$basepath/../memento/$timeu/memento/$time/${urlpath}/index.html" 1>/dev/null 2>/dev/null || mv -n "$basepath/../memento/$timeu/memento/$time/${urlpath}.html" "$basepath/../memento/$timeu/memento/$time/${urlpath}"
    if [[ "$urlpath" =~ : ]]; then urlpathcolon="$(echo "$urlpath" | sed "s/:/%3A/g")"; stat -t "$basepath/../memento/$timeu/memento/$time/${urlpathcolon}/index.html" 1>/dev/null 2>/dev/null || mv -n "$basepath/../memento/$timeu/memento/$time/${urlpathcolon}.html" "$basepath/../memento/$timeu/memento/$time/${urlpathcolon}"; fi

    filesf() { basedir="$basepath/../memento/$timeu"; basedirlen=$(expr $(echo -n "$basedir" | wc --bytes) + 1); find "$basedir" -type f | basedirlen="$basedirlen" xargs -d "\n" sh -c 'for args do nobasedir=$(echo "$args" | sed -E "s/^.{$basedirlen}//g"); echo " -F "file=@"\"$args\";filename=\"$nobasedir\"" | tr -d \\n; done' _; }
    filesf # debug output
    curl -k -X POST -H "Content-Type: multipart/form-data" $(filesf) "https://$localip:5001/api/v0/add?cid-version=1&chunker=size-1048576&recursive=true&wrap-with-directory=true&pin=false"
fi

# bug at https://$localip/cgi-bin/mem?url=https://archive.org/details/123 = archive.org has many files (CSS/JS/font slop) so the curl command fails. wget and curl works fine if wget -p grabbed not too many files.
# idea: make it work with .onion links too
